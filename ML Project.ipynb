{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01c27e00",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font>\n",
    "<div dir=ltr align=center>\n",
    "<img src=\"https://cdn.freebiesupply.com/logos/large/2x/sharif-logo-png-transparent.png\" width=150 height=150> <br>\n",
    "<font color=0F5298 size=7>\n",
    "    Machine learning <br>\n",
    "<font color=2565AE size=5>\n",
    "    Computer Engineering Department <br>\n",
    "    Fall 2025<br>\n",
    "<font color=3C99D size=5>\n",
    "    Sentiment Analysis with Transformer <br>\n",
    "</div>\n",
    "<div dir=ltr align=center>\n",
    "<font color=0CBCDF size=4>\n",
    "    Mohammad Ebrahimian, Taha Izadi, Nima Ghadirniya\n",
    "<font color=0CBCDF size=4>\n",
    "</div>\n",
    "\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e021900",
   "metadata": {},
   "source": [
    "  <h1 style=\"color:#0F5298; font-family:serif; font-size:45px; margin-bottom:0px;\">\n",
    "    Setup and Libraries\n",
    "  </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc32995",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim\n",
    "!pip install datasets==2.16.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45756247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import math\n",
    "import glob\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "import torch.optim as optim\n",
    "from itertools import product\n",
    "import torch.nn.functional as F\n",
    "import gensim.downloader as api\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from datasets import load_dataset\n",
    "from dataclasses import dataclass\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import BertTokenizer\n",
    "from typing import Dict, List, Optional\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.manual_seed(seed)\n",
    "\n",
    "set_seed(42)\n",
    "print(\"üå± Seed set to 42 for reproducibility.\")\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device set to: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5c57ee",
   "metadata": {},
   "source": [
    "  <h1 style=\"color:#0F5298; font-family:serif; font-size:45px; margin-bottom:0px;\">\n",
    "    Data Loading\n",
    "  </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8cd5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_file():\n",
    "    ds = load_dataset(\n",
    "        \"financial_phrasebank\",\n",
    "        \"sentences_allagree\",\n",
    "        trust_remote_code=True,\n",
    "        streaming=False\n",
    "    )\n",
    "    df = ds[\"train\"].to_pandas()\n",
    "    df = df.rename(columns={\"sentence\": \"text\"})\n",
    "\n",
    "    df = df.drop_duplicates(subset=['text'])\n",
    "    df = df.dropna(subset=['text', 'label'])\n",
    "\n",
    "    df['text'] = df['text'].str.lower().str.strip()\n",
    "\n",
    "    df = df.dropna(subset=['label'])\n",
    "    df['label'] = df['label'].astype(int)\n",
    "\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "def plot_sentiment_distribution(df):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    ax = sns.countplot(x='label', data=df, palette='viridis', hue='label', legend=False)\n",
    "    plt.title('Distribution of Sentiments (Sentences-AllAgree)')\n",
    "    plt.xlabel('Class (0: Neg, 1: Neu, 2: Pos)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks([0, 1, 2], ['Negative', 'Neutral', 'Positive'])\n",
    "\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                    ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
    "    plt.show()\n",
    "\n",
    "df = load_data_from_file()\n",
    "plot_sentiment_distribution(df)\n",
    "print(f\"Total unique samples: {len(df)}\")\n",
    "print(\"\\nüìù Samples per Class:\")\n",
    "label_names = {0: 'Negative', 1: 'Neutral', 2: 'Positive'}\n",
    "\n",
    "for label in [0, 1, 2]:\n",
    "    sample_text = df[df['label'] == label]['text'].iloc[0]\n",
    "    print(f\"   - {label_names[label]} (Label {label}): \\\"{sample_text[:100]}...\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44396ae5",
   "metadata": {},
   "source": [
    "  <h1 style=\"color:#0F5298; font-family:serif; font-size:45px; margin-bottom:0px;\">\n",
    "    Word2Vec Pre-trained embeding\n",
    "  </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ad92e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hybrid_embeddings(tokenizer, d_emb=300):\n",
    "    print(\"Loading Word2Vec model...\")\n",
    "    try:\n",
    "        word2vec = api.load(\"word2vec-google-news-300\")\n",
    "    except Exception as e:\n",
    "        print(f\"Word2Vec load failed: {e}\")\n",
    "        vocab_size = len(tokenizer)\n",
    "        mat = np.random.normal(0.0, 0.02, (vocab_size, d_emb)).astype(np.float32)\n",
    "        if tokenizer.pad_token_id is not None:\n",
    "            mat[tokenizer.pad_token_id] = 0.0\n",
    "        return torch.from_numpy(mat)\n",
    "\n",
    "    vocab = tokenizer.get_vocab()\n",
    "    vocab_size = len(vocab)\n",
    "    mat = np.random.normal(0.0, 0.02, (vocab_size, d_emb)).astype(np.float32)\n",
    "\n",
    "    special_ids = set(tokenizer.all_special_ids)\n",
    "    hits, misses, skipped_subword = 0, 0, 0\n",
    "\n",
    "    for token, idx in vocab.items():\n",
    "        if idx in special_ids:\n",
    "            continue\n",
    "        if token.startswith(\"##\"):\n",
    "            skipped_subword += 1\n",
    "            continue\n",
    "\n",
    "        if token in word2vec:\n",
    "            mat[idx] = word2vec[token]\n",
    "            hits += 1\n",
    "        elif token.lower() in word2vec:\n",
    "            mat[idx] = word2vec[token.lower()]\n",
    "            hits += 1\n",
    "        else:\n",
    "            misses += 1\n",
    "\n",
    "    if tokenizer.pad_token_id is not None:\n",
    "        mat[tokenizer.pad_token_id] = 0.0\n",
    "\n",
    "    del word2vec\n",
    "    gc.collect()\n",
    "\n",
    "    eligible = hits + misses\n",
    "    cov = (hits / eligible * 100) if eligible > 0 else 0.0\n",
    "    print(f\"Shape: {mat.shape}\")\n",
    "    print(f\"Hits: {hits}, Misses: {misses}, Skipped subwords: {skipped_subword}\")\n",
    "    print(f\"Coverage on eligible tokens: {cov:.1f}%\")\n",
    "\n",
    "    return torch.from_numpy(mat)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "pretrained_embeddings = create_hybrid_embeddings(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1263168",
   "metadata": {},
   "source": [
    "  <h1 style=\"color:#0F5298; font-family:serif; font-size:45px; margin-bottom:0px;\">\n",
    "    Data Spliting\n",
    "  </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fd312c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "TRAIN_SIZE = 0.80\n",
    "VAL_SIZE = 0.10\n",
    "TEST_SIZE = 0.10\n",
    "\n",
    "assert abs(TRAIN_SIZE + VAL_SIZE + TEST_SIZE - 1.0) < 1e-8, \"Split ratios must sum to 1.\"\n",
    "\n",
    "if df[\"label\"].dtype == object:\n",
    "    label_map = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "    df[\"label\"] = df[\"label\"].map(label_map)\n",
    "\n",
    "df = df.dropna(subset=[\"text\", \"label\"]).copy()\n",
    "df[\"label\"] = df[\"label\"].astype(int)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "train_df, temp_df = train_test_split(\n",
    "    df,\n",
    "    test_size=(1 - TRAIN_SIZE),\n",
    "    random_state=SEED,\n",
    "    stratify=df[\"label\"],\n",
    ")\n",
    "\n",
    "val_ratio_in_temp = VAL_SIZE / (VAL_SIZE + TEST_SIZE)\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=(1 - val_ratio_in_temp),\n",
    "    random_state=SEED,\n",
    "    stratify=temp_df[\"label\"],\n",
    ")\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "def show_split_stats(name, part_df):\n",
    "    counts = part_df[\"label\"].value_counts().sort_index()\n",
    "    ratios = (part_df[\"label\"].value_counts(normalize=True).sort_index() * 100).round(2)\n",
    "    print(f\"{name}: n={len(part_df)}\")\n",
    "    print(\"counts:\", counts.to_dict())\n",
    "    print(\"ratios(%):\", ratios.to_dict())\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "show_split_stats(\"Train\", train_df)\n",
    "show_split_stats(\"Validation\", val_df)\n",
    "show_split_stats(\"Test\", test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce149fd2",
   "metadata": {},
   "source": [
    "  <h1 style=\"color:#0F5298; font-family:serif; font-size:45px; margin-bottom:0px;\">\n",
    "    Augmentation\n",
    "  </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d03903",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMON_SYNONYMS = {\n",
    "    \"company\": [\"firm\", \"business\"],\n",
    "    \"market\": [\"sector\", \"marketplace\"],\n",
    "    \"shares\": [\"stock\", \"equity\"],\n",
    "    \"announced\": [\"reported\", \"stated\"],\n",
    "    \"increase\": [\"rise\", \"growth\"],\n",
    "    \"decrease\": [\"decline\", \"drop\"],\n",
    "    \"cost\": [\"expense\", \"charge\"],\n",
    "    \"revenue\": [\"sales\", \"turnover\"],\n",
    "}\n",
    "\n",
    "LABEL_SYNONYMS = {\n",
    "    0: {\"loss\": [\"deficit\", \"setback\"], \"risk\": [\"threat\", \"uncertainty\"], \"fall\": [\"drop\", \"decline\"]},\n",
    "    1: {\"said\": [\"stated\", \"noted\"], \"expects\": [\"anticipates\", \"foresees\"], \"plan\": [\"strategy\", \"program\"]},\n",
    "    2: {\"profit\": [\"gain\", \"earnings\"], \"growth\": [\"expansion\", \"rise\"], \"strong\": [\"solid\", \"robust\"]},\n",
    "}\n",
    "\n",
    "PROTECTED_WORDS = {\"not\", \"no\", \"never\", \"none\", \"without\"}\n",
    "\n",
    "\n",
    "def _normalize_token(token: str) -> str:\n",
    "    return re.sub(r\"^[^A-Za-z0-9]+|[^A-Za-z0-9]+$\", \"\", token).lower()\n",
    "\n",
    "\n",
    "def _replace_token_keep_format(raw_token: str, new_core: str) -> str:\n",
    "    m = re.match(r\"^([^A-Za-z0-9]*)([A-Za-z0-9'-]+)([^A-Za-z0-9]*)$\", raw_token)\n",
    "    if not m:\n",
    "        return raw_token\n",
    "    prefix, core, suffix = m.groups()\n",
    "    if core.isupper():\n",
    "        new_core = new_core.upper()\n",
    "    elif core[:1].isupper():\n",
    "        new_core = new_core.capitalize()\n",
    "    return f\"{prefix}{new_core}{suffix}\"\n",
    "\n",
    "\n",
    "def augment_text_label_aware(text: str, label: int, rng: random.Random, max_repl: int = 2, swap_prob: float = 0.10):\n",
    "    words = text.split()\n",
    "    if len(words) < 3:\n",
    "        return text\n",
    "\n",
    "    syn_map = {**COMMON_SYNONYMS, **LABEL_SYNONYMS.get(int(label), {})}\n",
    "    candidates = []\n",
    "\n",
    "    for i, w in enumerate(words):\n",
    "        key = _normalize_token(w)\n",
    "        if not key or key in PROTECTED_WORDS or any(ch.isdigit() for ch in key):\n",
    "            continue\n",
    "        if key in syn_map:\n",
    "            candidates.append((i, key))\n",
    "\n",
    "    rng.shuffle(candidates)\n",
    "\n",
    "    if candidates:\n",
    "        n_rep = rng.randint(1, min(max_repl, len(candidates)))\n",
    "        for i, key in candidates[:n_rep]:\n",
    "            replacement = rng.choice(syn_map[key])\n",
    "            words[i] = _replace_token_keep_format(words[i], replacement)\n",
    "\n",
    "    if rng.random() < swap_prob and len(words) >= 5:\n",
    "        j = rng.randrange(0, len(words) - 1)\n",
    "        words[j], words[j + 1] = words[j + 1], words[j]\n",
    "\n",
    "    aug = \" \".join(words).strip()\n",
    "    return aug if aug else text\n",
    "\n",
    "\n",
    "def _compute_target_counts(class_counts: pd.Series, balance_strength: float = 0.45, max_growth: float = 1.40):\n",
    "    class_counts = class_counts.sort_index()\n",
    "    max_count = int(class_counts.max())\n",
    "    orig_total = int(class_counts.sum())\n",
    "\n",
    "    targets = {}\n",
    "    for label, count in class_counts.items():\n",
    "        boosted = int(round(count + balance_strength * (max_count - count)))\n",
    "        targets[int(label)] = max(int(count), boosted)\n",
    "\n",
    "    max_total = int(round(orig_total * max_growth))\n",
    "    proposed_total = sum(targets.values())\n",
    "\n",
    "    if proposed_total > max_total and proposed_total > orig_total:\n",
    "        proposed_extra = proposed_total - orig_total\n",
    "        allowed_extra = max_total - orig_total\n",
    "        scale = allowed_extra / proposed_extra if proposed_extra > 0 else 0.0\n",
    "        for label, count in class_counts.items():\n",
    "            extra = targets[int(label)] - int(count)\n",
    "            scaled_extra = int(round(extra * scale))\n",
    "            targets[int(label)] = int(count) + max(0, scaled_extra)\n",
    "\n",
    "    return targets\n",
    "\n",
    "# Augmenting training data\n",
    "def build_controlled_augmented_train_df(\n",
    "    train_df: pd.DataFrame,\n",
    "    seed: int = 42,\n",
    "    balance_strength: float = 1.0,\n",
    "    max_growth: float = 10.0,\n",
    "):\n",
    "    rng = random.Random(seed)\n",
    "\n",
    "    base = train_df[[\"text\", \"label\"]].copy().reset_index(drop=True)\n",
    "    base[\"is_augmented\"] = 0\n",
    "\n",
    "    class_counts = base[\"label\"].value_counts().sort_index()\n",
    "    target_counts = _compute_target_counts(class_counts, balance_strength=balance_strength, max_growth=max_growth)\n",
    "\n",
    "    parts = []\n",
    "    for label, grp in base.groupby(\"label\", sort=True):\n",
    "        grp = grp.copy().reset_index(drop=True)\n",
    "        originals = grp[\"text\"].tolist()\n",
    "        seen = set(t.strip().lower() for t in originals)\n",
    "\n",
    "        need = max(0, target_counts[int(label)] - len(grp))\n",
    "        new_rows = []\n",
    "        attempts = 0\n",
    "        max_attempts = max(200, need * 20)\n",
    "\n",
    "        while len(new_rows) < need and attempts < max_attempts:\n",
    "            src = originals[rng.randrange(len(originals))]\n",
    "            aug = augment_text_label_aware(src, int(label), rng, max_repl=2, swap_prob=0.10)\n",
    "            attempts += 1\n",
    "\n",
    "            key = aug.strip().lower()\n",
    "            if not key or key in seen:\n",
    "                continue\n",
    "\n",
    "            seen.add(key)\n",
    "            new_rows.append({\"text\": aug, \"label\": int(label), \"is_augmented\": 1})\n",
    "\n",
    "        if len(new_rows) < need:\n",
    "            remain = need - len(new_rows)\n",
    "            sampled = grp.sample(n=remain, replace=True, random_state=seed)[\"text\"].tolist()\n",
    "            for src in sampled:\n",
    "                aug = augment_text_label_aware(src, int(label), rng, max_repl=1, swap_prob=0.05)\n",
    "                new_rows.append({\"text\": aug, \"label\": int(label), \"is_augmented\": 1})\n",
    "\n",
    "        parts.append(grp)\n",
    "        if new_rows:\n",
    "            parts.append(pd.DataFrame(new_rows))\n",
    "\n",
    "    train_aug_df = pd.concat(parts, ignore_index=True)\n",
    "    train_aug_df = train_aug_df.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
    "    return train_aug_df\n",
    "\n",
    "\n",
    "def show_counts(train_part, val_part, test_part, title):\n",
    "    print(f\"\\n{title}\")\n",
    "    print(\"=\" * len(title))\n",
    "    for name, part in [(\"Train\", train_part), (\"Validation\", val_part), (\"Test\", test_part)]:\n",
    "        c = part[\"label\"].value_counts().sort_index().to_dict()\n",
    "        print(f\"{name}: n={len(part)} | class_counts={c}\")\n",
    "\n",
    "\n",
    "assert \"train_df\" in globals() and \"val_df\" in globals() and \"test_df\" in globals(), \"run data spliting block first\"\n",
    "\n",
    "seed_value = SEED if \"SEED\" in globals() else 42\n",
    "\n",
    "show_counts(train_df, val_df, test_df, \"Before Augmentation\")\n",
    "\n",
    "train_aug_df = build_controlled_augmented_train_df(\n",
    "    train_df=train_df,\n",
    "    seed=seed_value,\n",
    "    balance_strength=1,\n",
    "    max_growth=3,\n",
    ")\n",
    "\n",
    "show_counts(train_aug_df, val_df, test_df, \"After Augmentation\")\n",
    "print(f\"Added train samples: {len(train_aug_df) - len(train_df)}\")\n",
    "print(\"Augmented flag:\", train_aug_df[\"is_augmented\"].value_counts().to_dict())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6a843c",
   "metadata": {},
   "source": [
    "  <h1 style=\"color:#0F5298; font-family:serif; font-size:45px; margin-bottom:0px;\">\n",
    "    Data loader\n",
    "  </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a993c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinancialDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=64):\n",
    "        self.texts = list(texts)\n",
    "        self.labels = list(labels)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = int(self.labels[idx])\n",
    "\n",
    "        enc = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "MAX_LEN = 64\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = FinancialDataset(train_aug_df[\"text\"].values, train_aug_df[\"label\"].values, tokenizer, MAX_LEN)\n",
    "val_dataset   = FinancialDataset(val_df[\"text\"].values, val_df[\"label\"].values, tokenizer, MAX_LEN)\n",
    "test_dataset  = FinancialDataset(test_df[\"text\"].values, test_df[\"label\"].values, tokenizer, MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)} | batches: {len(train_loader)}\")\n",
    "print(f\"Val samples:   {len(val_dataset)} | batches: {len(val_loader)}\")\n",
    "print(f\"Test samples:  {len(test_dataset)} | batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b32473",
   "metadata": {},
   "source": [
    "  <h1 style=\"color:#0F5298; font-family:serif; font-size:45px; margin-bottom:0px;\">\n",
    "    The Architecture (Transformer)\n",
    "  </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b421f3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class ModelConfig:\n",
    "    vocab_size: int\n",
    "    num_classes: int\n",
    "    max_len: int\n",
    "    d_model: int\n",
    "    num_heads: int\n",
    "    num_layers: int\n",
    "    d_ff: int\n",
    "    dropout: float\n",
    "    attn_dropout: float\n",
    "    pad_token_id: int = 0\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, attn_dropout: float, proj_dropout: float):\n",
    "        super().__init__()\n",
    "        if d_model % num_heads != 0:\n",
    "            raise ValueError(\"d_model must be divisible by num_heads\")\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(d_model, 3 * d_model, bias=True)\n",
    "        self.attn_drop = nn.Dropout(attn_dropout)\n",
    "        self.proj = nn.Linear(d_model, d_model, bias=True)\n",
    "        self.proj_drop = nn.Dropout(proj_dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attention_mask: Optional[torch.Tensor] = None):\n",
    "        bsz, seq_len, _ = x.shape\n",
    "        qkv = self.qkv(x).view(bsz, seq_len, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            key_mask = attention_mask[:, None, None, :].to(torch.bool)\n",
    "            scores = scores.masked_fill(~key_mask, torch.finfo(scores.dtype).min)\n",
    "\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        context = torch.matmul(attn, v).transpose(1, 2).contiguous().view(bsz, seq_len, self.d_model)\n",
    "        out = self.proj_drop(self.proj(context))\n",
    "        return out, attn\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.fc1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float, attn_dropout: float):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.attn = MultiHeadSelfAttention(d_model, num_heads, attn_dropout=attn_dropout, proj_dropout=dropout)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.ffn = FeedForward(d_model, d_ff, dropout=dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attention_mask: Optional[torch.Tensor] = None):\n",
    "        h, attn = self.attn(self.norm1(x), attention_mask)\n",
    "        x = x + h\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "        return x, attn\n",
    "\n",
    "\n",
    "class FinancialTransformer(nn.Module):\n",
    "    def __init__(self, config: ModelConfig, pretrained_embeddings: Optional[torch.Tensor] = None):\n",
    "        super().__init__()\n",
    "        emb_dim = pretrained_embeddings.size(1) if pretrained_embeddings is not None else config.d_model\n",
    "\n",
    "        self.token_embedding = nn.Embedding(config.vocab_size, emb_dim, padding_idx=config.pad_token_id)\n",
    "        self.input_proj = nn.Identity() if emb_dim == config.d_model else nn.Linear(emb_dim, config.d_model, bias=False)\n",
    "        self.position_embedding = nn.Embedding(config.max_len, config.d_model)\n",
    "        self.embed_norm = nn.LayerNorm(config.d_model)\n",
    "        self.embed_drop = nn.Dropout(config.dropout)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerEncoderBlock(\n",
    "                    d_model=config.d_model,\n",
    "                    num_heads=config.num_heads,\n",
    "                    d_ff=config.d_ff,\n",
    "                    dropout=config.dropout,\n",
    "                    attn_dropout=config.attn_dropout,\n",
    "                )\n",
    "                for _ in range(config.num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.final_norm = nn.LayerNorm(config.d_model)\n",
    "        self.cls_drop = nn.Dropout(config.dropout)\n",
    "        self.classifier = nn.Linear(config.d_model, config.num_classes)\n",
    "\n",
    "        has_pretrained = pretrained_embeddings is not None\n",
    "        self._init_weights(skip_token_embedding=has_pretrained)\n",
    "\n",
    "        if has_pretrained:\n",
    "            if pretrained_embeddings.size(0) != config.vocab_size:\n",
    "                raise ValueError(\"pretrained_embeddings vocab_size mismatch\")\n",
    "            with torch.no_grad():\n",
    "                self.token_embedding.weight.copy_(pretrained_embeddings)\n",
    "                if self.token_embedding.padding_idx is not None:\n",
    "                    self.token_embedding.weight[self.token_embedding.padding_idx].zero_()\n",
    "\n",
    "    def _init_weights(self, skip_token_embedding: bool = False):\n",
    "        for name, m in self.named_modules():\n",
    "            if skip_token_embedding and name == \"token_embedding\":\n",
    "                continue\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Embedding):\n",
    "                nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "                if m.padding_idx is not None:\n",
    "                    with torch.no_grad():\n",
    "                        m.weight[m.padding_idx].zero_()\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor] = None, return_attention: bool = False):\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones_like(input_ids)\n",
    "\n",
    "        bsz, seq_len = input_ids.shape\n",
    "        if seq_len > self.position_embedding.num_embeddings:\n",
    "            raise ValueError(\"seq_len exceeds max_len in config\")\n",
    "\n",
    "        pos_ids = torch.arange(seq_len, device=input_ids.device).unsqueeze(0).expand(bsz, seq_len)\n",
    "\n",
    "        x = self.token_embedding(input_ids)\n",
    "        x = self.input_proj(x)\n",
    "        x = x + self.position_embedding(pos_ids)\n",
    "        x = self.embed_norm(x)\n",
    "        x = self.embed_drop(x)\n",
    "\n",
    "        attn_maps = []\n",
    "        for layer in self.layers:\n",
    "            x, attn = layer(x, attention_mask)\n",
    "            if return_attention:\n",
    "                attn_maps.append(attn)\n",
    "\n",
    "        x = self.final_norm(x)\n",
    "        mask = attention_mask.unsqueeze(-1).type_as(x)\n",
    "        pooled = (x * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1.0)\n",
    "        logits = self.classifier(self.cls_drop(pooled))\n",
    "\n",
    "        if return_attention:\n",
    "            return logits, attn_maps\n",
    "        return logits\n",
    "\n",
    "\n",
    "DEFAULT_SEARCH_SPACE = {\n",
    "    \"d_model\": [128, 192, 256],\n",
    "    \"num_heads\": [4, 8],\n",
    "    \"num_layers\": [3, 4],\n",
    "    \"ff_mult\": [3],\n",
    "    \"dropout\": [0.1, 0.2, 0.3],\n",
    "    \"attn_dropout\": [0.1],\n",
    "}\n",
    "\n",
    "\n",
    "def generate_valid_model_configs(\n",
    "    vocab_size: int,\n",
    "    num_classes: int,\n",
    "    max_len: int,\n",
    "    pad_token_id: int = 0,\n",
    "    search_space: Optional[Dict[str, List]] = None,\n",
    ") -> List[ModelConfig]:\n",
    "    space = search_space or DEFAULT_SEARCH_SPACE\n",
    "    configs: List[ModelConfig] = []\n",
    "\n",
    "    for d_model, num_heads, num_layers, ff_mult, dropout, attn_dropout in product(\n",
    "        space[\"d_model\"],\n",
    "        space[\"num_heads\"],\n",
    "        space[\"num_layers\"],\n",
    "        space[\"ff_mult\"],\n",
    "        space[\"dropout\"],\n",
    "        space[\"attn_dropout\"],\n",
    "    ):\n",
    "        if d_model % num_heads != 0:\n",
    "            continue\n",
    "\n",
    "        cfg = ModelConfig(\n",
    "            vocab_size=vocab_size,\n",
    "            num_classes=num_classes,\n",
    "            max_len=max_len,\n",
    "            d_model=int(d_model),\n",
    "            num_heads=int(num_heads),\n",
    "            num_layers=int(num_layers),\n",
    "            d_ff=int(d_model * ff_mult),\n",
    "            dropout=float(dropout),\n",
    "            attn_dropout=float(attn_dropout),\n",
    "            pad_token_id=int(pad_token_id),\n",
    "        )\n",
    "        configs.append(cfg)\n",
    "\n",
    "    return configs\n",
    "\n",
    "\n",
    "def build_model_from_config(\n",
    "    config: ModelConfig,\n",
    "    pretrained_embeddings: Optional[torch.Tensor] = None,\n",
    "    device: Optional[torch.device] = None,\n",
    ") -> FinancialTransformer:\n",
    "    model = FinancialTransformer(config=config, pretrained_embeddings=pretrained_embeddings)\n",
    "    if device is not None:\n",
    "        model = model.to(device)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038de815",
   "metadata": {},
   "source": [
    "  <h1 style=\"color:#0F5298; font-family:serif; font-size:45px; margin-bottom:0px;\">\n",
    "    Generate Model\n",
    "  </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab055967",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = generate_valid_model_configs(\n",
    "    vocab_size=len(tokenizer),\n",
    "    num_classes=3,\n",
    "    max_len=MAX_LEN,\n",
    "    pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0\n",
    ")\n",
    "\n",
    "print(f\"Total valid configs: {len(configs)}\")\n",
    "print(\"First config:\", configs[0])\n",
    "\n",
    "model = build_model_from_config(\n",
    "    config=configs[0],\n",
    "    pretrained_embeddings=pretrained_embeddings if \"pretrained_embeddings\" in globals() else None,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"Model params: {sum(p.numel() for p in model.parameters()):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2673c460",
   "metadata": {},
   "source": [
    "  <h1 style=\"color:#0F5298; font-family:serif; font-size:45px; margin-bottom:0px;\">\n",
    "    Training Configuration & Class Weights\n",
    "  </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcdd044",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 15\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 5e-4\n",
    "LABEL_SMOOTHING = 0.03\n",
    "BETAS = (0.9, 0.999)\n",
    "EPS = 1e-8\n",
    "WARMUP_RATIO = 0.1\n",
    "MAX_GRAD_NORM = 1.0\n",
    "EARLY_STOPPING_PATIENCE = 4\n",
    "MIN_IMPROVEMENT = 5e-4\n",
    "FREEZE_EMBED_EPOCHS = 2\n",
    "SELECTION_METRIC = \"macro_f1\"\n",
    "MAX_CONFIGS = 5\n",
    "\n",
    "if \"train_aug_df\" in globals():\n",
    "    y_train = train_aug_df[\"label\"].astype(int).to_numpy()\n",
    "elif \"train_df\" in globals():\n",
    "    y_train = train_df[\"label\"].astype(int).to_numpy()\n",
    "elif \"train_loader\" in globals():\n",
    "    labels_buffer = []\n",
    "    for b in train_loader:\n",
    "        labels_buffer.extend(b[\"labels\"].cpu().numpy().tolist())\n",
    "    y_train = np.array(labels_buffer, dtype=np.int64)\n",
    "else:\n",
    "    raise ValueError(\"No training labels found. Define train_df/train_aug_df or train_loader first.\")\n",
    "\n",
    "num_classes = 3\n",
    "weights_np = compute_class_weight(class_weight=\"balanced\", classes=np.arange(num_classes), y=y_train)\n",
    "class_weights = torch.tensor(weights_np, dtype=torch.float32, device=device)\n",
    "\n",
    "training_config = {\n",
    "    \"epochs\": NUM_EPOCHS,\n",
    "    \"lr\": LEARNING_RATE,\n",
    "    \"weight_decay\": WEIGHT_DECAY,\n",
    "    \"label_smoothing\": LABEL_SMOOTHING,\n",
    "    \"betas\": BETAS,\n",
    "    \"eps\": EPS,\n",
    "    \"warmup_ratio\": WARMUP_RATIO,\n",
    "    \"max_grad_norm\": MAX_GRAD_NORM,\n",
    "    \"early_stopping_patience\": EARLY_STOPPING_PATIENCE,\n",
    "    \"min_improvement\": MIN_IMPROVEMENT,\n",
    "    \"freeze_embed_epochs\": FREEZE_EMBED_EPOCHS,\n",
    "    \"selection_metric\": SELECTION_METRIC,\n",
    "    \"max_configs\": MAX_CONFIGS,\n",
    "}\n",
    "\n",
    "print(\"Training config ready.\")\n",
    "print(training_config)\n",
    "print(\"Class weights:\", class_weights.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3583dfea",
   "metadata": {},
   "source": [
    "  <h1 style=\"color:#0F5298; font-family:serif; font-size:45px; margin-bottom:0px;\">\n",
    "    Helpers\n",
    "  </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a38860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _seed_all(seed: int = 42):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def _get_train_labels():\n",
    "    if \"train_aug_df\" in globals():\n",
    "        return train_aug_df[\"label\"].astype(int).to_numpy()\n",
    "    if \"train_df\" in globals():\n",
    "        return train_df[\"label\"].astype(int).to_numpy()\n",
    "    if \"train_loader\" in globals():\n",
    "        labels = []\n",
    "        for b in train_loader:\n",
    "            labels.extend(b[\"labels\"].cpu().numpy().tolist())\n",
    "        return np.array(labels, dtype=np.int64)\n",
    "    raise ValueError(\"No training labels found.\")\n",
    "\n",
    "\n",
    "def _build_scheduler(optimizer, num_epochs, steps_per_epoch, warmup_ratio):\n",
    "    total_steps = max(1, num_epochs * steps_per_epoch)\n",
    "    warmup_steps = int(warmup_ratio * total_steps)\n",
    "\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < warmup_steps:\n",
    "            return float(current_step) / float(max(1, warmup_steps))\n",
    "        progress = float(current_step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "\n",
    "    return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda), total_steps, warmup_steps\n",
    "\n",
    "\n",
    "def _set_token_embedding_trainable(model, trainable: bool):\n",
    "    if hasattr(model, \"token_embedding\"):\n",
    "        for p in model.token_embedding.parameters():\n",
    "            p.requires_grad = trainable\n",
    "\n",
    "\n",
    "def _train_one_epoch(model, loader, criterion, optimizer, scheduler, max_grad_norm, device):\n",
    "    model.train()\n",
    "    total_loss, total_correct, total_samples = 0.0, 0, 0\n",
    "\n",
    "    for batch in loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        if max_grad_norm is not None and max_grad_norm > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        bs = labels.size(0)\n",
    "        total_loss += loss.item() * bs\n",
    "        total_correct += (logits.argmax(dim=1) == labels).sum().item()\n",
    "        total_samples += bs\n",
    "\n",
    "    return total_loss / max(1, total_samples), total_correct / max(1, total_samples)\n",
    "\n",
    "test_labels = []\n",
    "test_preds = []\n",
    "@torch.no_grad()\n",
    "def _eval_one_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, total_correct, total_samples = 0.0, 0, 0\n",
    "    all_labels, all_preds = [], []\n",
    "\n",
    "    for batch in loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        preds = logits.argmax(dim=1)\n",
    "\n",
    "        bs = labels.size(0)\n",
    "        total_loss += loss.item() * bs\n",
    "        total_correct += (preds == labels).sum().item()\n",
    "        total_samples += bs\n",
    "\n",
    "        all_labels.extend(labels.detach().cpu().numpy().tolist())\n",
    "        all_preds.extend(preds.detach().cpu().numpy().tolist())\n",
    "\n",
    "        test_labels.extend(labels.detach().cpu().numpy().tolist())\n",
    "        test_preds.extend(preds.detach().cpu().numpy().tolist())\n",
    "\n",
    "    macro_f1 = f1_score(all_labels, all_preds, average=\"macro\", zero_division=0)\n",
    "    return total_loss / max(1, total_samples), total_correct / max(1, total_samples), macro_f1\n",
    "\n",
    "\n",
    "def _is_improved(metric_name, current_loss, current_f1, best_loss, best_f1, min_improvement):\n",
    "    if metric_name == \"val_loss\":\n",
    "        return current_loss < (best_loss - min_improvement)\n",
    "    if current_f1 > (best_f1 + min_improvement):\n",
    "        return True\n",
    "    if abs(current_f1 - best_f1) <= 1e-12 and current_loss < best_loss:\n",
    "        return True\n",
    "    return False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
