{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01c27e00",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font>\n",
    "<div dir=ltr align=center>\n",
    "<img src=\"https://cdn.freebiesupply.com/logos/large/2x/sharif-logo-png-transparent.png\" width=150 height=150> <br>\n",
    "<font color=0F5298 size=7>\n",
    "    Machine learning <br>\n",
    "<font color=2565AE size=5>\n",
    "    Computer Engineering Department <br>\n",
    "    Fall 2025<br>\n",
    "<font color=3C99D size=5>\n",
    "    Sentiment Analysis with Transformer <br>\n",
    "</div>\n",
    "<div dir=ltr align=center>\n",
    "<font color=0CBCDF size=4>\n",
    "    Mohammad Ebrahimian, Taha Izadi, Nima Ghadirniya\n",
    "<font color=0CBCDF size=4>\n",
    "</div>\n",
    "\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e021900",
   "metadata": {},
   "source": [
    "  <h1 style=\"color:#0F5298; font-family:serif; font-size:45px; margin-bottom:0px;\">\n",
    "    Setup and Libraries\n",
    "  </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc32995",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim\n",
    "!pip install datasets==2.16.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45756247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import math\n",
    "import glob\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "import torch.optim as optim\n",
    "from itertools import product\n",
    "import torch.nn.functional as F\n",
    "import gensim.downloader as api\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from datasets import load_dataset\n",
    "from dataclasses import dataclass\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import BertTokenizer\n",
    "from typing import Dict, List, Optional\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.manual_seed(seed)\n",
    "\n",
    "set_seed(42)\n",
    "print(\"üå± Seed set to 42 for reproducibility.\")\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device set to: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5c57ee",
   "metadata": {},
   "source": [
    "  <h1 style=\"color:#0F5298; font-family:serif; font-size:45px; margin-bottom:0px;\">\n",
    "    Data Loading\n",
    "  </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8cd5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_file():\n",
    "    ds = load_dataset(\n",
    "        \"financial_phrasebank\",\n",
    "        \"sentences_allagree\",\n",
    "        trust_remote_code=True,\n",
    "        streaming=False\n",
    "    )\n",
    "    df = ds[\"train\"].to_pandas()\n",
    "    df = df.rename(columns={\"sentence\": \"text\"})\n",
    "\n",
    "    df = df.drop_duplicates(subset=['text'])\n",
    "    df = df.dropna(subset=['text', 'label'])\n",
    "\n",
    "    df['text'] = df['text'].str.lower().str.strip()\n",
    "\n",
    "    df = df.dropna(subset=['label'])\n",
    "    df['label'] = df['label'].astype(int)\n",
    "\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "def plot_sentiment_distribution(df):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    ax = sns.countplot(x='label', data=df, palette='viridis', hue='label', legend=False)\n",
    "    plt.title('Distribution of Sentiments (Sentences-AllAgree)')\n",
    "    plt.xlabel('Class (0: Neg, 1: Neu, 2: Pos)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks([0, 1, 2], ['Negative', 'Neutral', 'Positive'])\n",
    "\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                    ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
    "    plt.show()\n",
    "\n",
    "df = load_data_from_file()\n",
    "plot_sentiment_distribution(df)\n",
    "print(f\"Total unique samples: {len(df)}\")\n",
    "print(\"\\nüìù Samples per Class:\")\n",
    "label_names = {0: 'Negative', 1: 'Neutral', 2: 'Positive'}\n",
    "\n",
    "for label in [0, 1, 2]:\n",
    "    sample_text = df[df['label'] == label]['text'].iloc[0]\n",
    "    print(f\"   - {label_names[label]} (Label {label}): \\\"{sample_text[:100]}...\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44396ae5",
   "metadata": {},
   "source": [
    "  <h1 style=\"color:#0F5298; font-family:serif; font-size:45px; margin-bottom:0px;\">\n",
    "    Word2Vec Pre-trained embeding\n",
    "  </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ad92e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hybrid_embeddings(tokenizer, d_emb=300):\n",
    "    print(\"Loading Word2Vec model...\")\n",
    "    try:\n",
    "        word2vec = api.load(\"word2vec-google-news-300\")\n",
    "    except Exception as e:\n",
    "        print(f\"Word2Vec load failed: {e}\")\n",
    "        vocab_size = len(tokenizer)\n",
    "        mat = np.random.normal(0.0, 0.02, (vocab_size, d_emb)).astype(np.float32)\n",
    "        if tokenizer.pad_token_id is not None:\n",
    "            mat[tokenizer.pad_token_id] = 0.0\n",
    "        return torch.from_numpy(mat)\n",
    "\n",
    "    vocab = tokenizer.get_vocab()\n",
    "    vocab_size = len(vocab)\n",
    "    mat = np.random.normal(0.0, 0.02, (vocab_size, d_emb)).astype(np.float32)\n",
    "\n",
    "    special_ids = set(tokenizer.all_special_ids)\n",
    "    hits, misses, skipped_subword = 0, 0, 0\n",
    "\n",
    "    for token, idx in vocab.items():\n",
    "        if idx in special_ids:\n",
    "            continue\n",
    "        if token.startswith(\"##\"):\n",
    "            skipped_subword += 1\n",
    "            continue\n",
    "\n",
    "        if token in word2vec:\n",
    "            mat[idx] = word2vec[token]\n",
    "            hits += 1\n",
    "        elif token.lower() in word2vec:\n",
    "            mat[idx] = word2vec[token.lower()]\n",
    "            hits += 1\n",
    "        else:\n",
    "            misses += 1\n",
    "\n",
    "    if tokenizer.pad_token_id is not None:\n",
    "        mat[tokenizer.pad_token_id] = 0.0\n",
    "\n",
    "    del word2vec\n",
    "    gc.collect()\n",
    "\n",
    "    eligible = hits + misses\n",
    "    cov = (hits / eligible * 100) if eligible > 0 else 0.0\n",
    "    print(f\"Shape: {mat.shape}\")\n",
    "    print(f\"Hits: {hits}, Misses: {misses}, Skipped subwords: {skipped_subword}\")\n",
    "    print(f\"Coverage on eligible tokens: {cov:.1f}%\")\n",
    "\n",
    "    return torch.from_numpy(mat)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "pretrained_embeddings = create_hybrid_embeddings(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1263168",
   "metadata": {},
   "source": [
    "  <h1 style=\"color:#0F5298; font-family:serif; font-size:45px; margin-bottom:0px;\">\n",
    "    Data Spliting\n",
    "  </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fd312c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "TRAIN_SIZE = 0.80\n",
    "VAL_SIZE = 0.10\n",
    "TEST_SIZE = 0.10\n",
    "\n",
    "assert abs(TRAIN_SIZE + VAL_SIZE + TEST_SIZE - 1.0) < 1e-8, \"Split ratios must sum to 1.\"\n",
    "\n",
    "if df[\"label\"].dtype == object:\n",
    "    label_map = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "    df[\"label\"] = df[\"label\"].map(label_map)\n",
    "\n",
    "df = df.dropna(subset=[\"text\", \"label\"]).copy()\n",
    "df[\"label\"] = df[\"label\"].astype(int)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "train_df, temp_df = train_test_split(\n",
    "    df,\n",
    "    test_size=(1 - TRAIN_SIZE),\n",
    "    random_state=SEED,\n",
    "    stratify=df[\"label\"],\n",
    ")\n",
    "\n",
    "val_ratio_in_temp = VAL_SIZE / (VAL_SIZE + TEST_SIZE)\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=(1 - val_ratio_in_temp),\n",
    "    random_state=SEED,\n",
    "    stratify=temp_df[\"label\"],\n",
    ")\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "def show_split_stats(name, part_df):\n",
    "    counts = part_df[\"label\"].value_counts().sort_index()\n",
    "    ratios = (part_df[\"label\"].value_counts(normalize=True).sort_index() * 100).round(2)\n",
    "    print(f\"{name}: n={len(part_df)}\")\n",
    "    print(\"counts:\", counts.to_dict())\n",
    "    print(\"ratios(%):\", ratios.to_dict())\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "show_split_stats(\"Train\", train_df)\n",
    "show_split_stats(\"Validation\", val_df)\n",
    "show_split_stats(\"Test\", test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce149fd2",
   "metadata": {},
   "source": [
    "  <h1 style=\"color:#0F5298; font-family:serif; font-size:45px; margin-bottom:0px;\">\n",
    "    Augmentation\n",
    "  </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d03903",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMON_SYNONYMS = {\n",
    "    \"company\": [\"firm\", \"business\"],\n",
    "    \"market\": [\"sector\", \"marketplace\"],\n",
    "    \"shares\": [\"stock\", \"equity\"],\n",
    "    \"announced\": [\"reported\", \"stated\"],\n",
    "    \"increase\": [\"rise\", \"growth\"],\n",
    "    \"decrease\": [\"decline\", \"drop\"],\n",
    "    \"cost\": [\"expense\", \"charge\"],\n",
    "    \"revenue\": [\"sales\", \"turnover\"],\n",
    "}\n",
    "\n",
    "LABEL_SYNONYMS = {\n",
    "    0: {\"loss\": [\"deficit\", \"setback\"], \"risk\": [\"threat\", \"uncertainty\"], \"fall\": [\"drop\", \"decline\"]},\n",
    "    1: {\"said\": [\"stated\", \"noted\"], \"expects\": [\"anticipates\", \"foresees\"], \"plan\": [\"strategy\", \"program\"]},\n",
    "    2: {\"profit\": [\"gain\", \"earnings\"], \"growth\": [\"expansion\", \"rise\"], \"strong\": [\"solid\", \"robust\"]},\n",
    "}\n",
    "\n",
    "PROTECTED_WORDS = {\"not\", \"no\", \"never\", \"none\", \"without\"}\n",
    "\n",
    "\n",
    "def _normalize_token(token: str) -> str:\n",
    "    return re.sub(r\"^[^A-Za-z0-9]+|[^A-Za-z0-9]+$\", \"\", token).lower()\n",
    "\n",
    "\n",
    "def _replace_token_keep_format(raw_token: str, new_core: str) -> str:\n",
    "    m = re.match(r\"^([^A-Za-z0-9]*)([A-Za-z0-9'-]+)([^A-Za-z0-9]*)$\", raw_token)\n",
    "    if not m:\n",
    "        return raw_token\n",
    "    prefix, core, suffix = m.groups()\n",
    "    if core.isupper():\n",
    "        new_core = new_core.upper()\n",
    "    elif core[:1].isupper():\n",
    "        new_core = new_core.capitalize()\n",
    "    return f\"{prefix}{new_core}{suffix}\"\n",
    "\n",
    "\n",
    "def augment_text_label_aware(text: str, label: int, rng: random.Random, max_repl: int = 2, swap_prob: float = 0.10):\n",
    "    words = text.split()\n",
    "    if len(words) < 3:\n",
    "        return text\n",
    "\n",
    "    syn_map = {**COMMON_SYNONYMS, **LABEL_SYNONYMS.get(int(label), {})}\n",
    "    candidates = []\n",
    "\n",
    "    for i, w in enumerate(words):\n",
    "        key = _normalize_token(w)\n",
    "        if not key or key in PROTECTED_WORDS or any(ch.isdigit() for ch in key):\n",
    "            continue\n",
    "        if key in syn_map:\n",
    "            candidates.append((i, key))\n",
    "\n",
    "    rng.shuffle(candidates)\n",
    "\n",
    "    if candidates:\n",
    "        n_rep = rng.randint(1, min(max_repl, len(candidates)))\n",
    "        for i, key in candidates[:n_rep]:\n",
    "            replacement = rng.choice(syn_map[key])\n",
    "            words[i] = _replace_token_keep_format(words[i], replacement)\n",
    "\n",
    "    if rng.random() < swap_prob and len(words) >= 5:\n",
    "        j = rng.randrange(0, len(words) - 1)\n",
    "        words[j], words[j + 1] = words[j + 1], words[j]\n",
    "\n",
    "    aug = \" \".join(words).strip()\n",
    "    return aug if aug else text\n",
    "\n",
    "\n",
    "def _compute_target_counts(class_counts: pd.Series, balance_strength: float = 0.45, max_growth: float = 1.40):\n",
    "    class_counts = class_counts.sort_index()\n",
    "    max_count = int(class_counts.max())\n",
    "    orig_total = int(class_counts.sum())\n",
    "\n",
    "    targets = {}\n",
    "    for label, count in class_counts.items():\n",
    "        boosted = int(round(count + balance_strength * (max_count - count)))\n",
    "        targets[int(label)] = max(int(count), boosted)\n",
    "\n",
    "    max_total = int(round(orig_total * max_growth))\n",
    "    proposed_total = sum(targets.values())\n",
    "\n",
    "    if proposed_total > max_total and proposed_total > orig_total:\n",
    "        proposed_extra = proposed_total - orig_total\n",
    "        allowed_extra = max_total - orig_total\n",
    "        scale = allowed_extra / proposed_extra if proposed_extra > 0 else 0.0\n",
    "        for label, count in class_counts.items():\n",
    "            extra = targets[int(label)] - int(count)\n",
    "            scaled_extra = int(round(extra * scale))\n",
    "            targets[int(label)] = int(count) + max(0, scaled_extra)\n",
    "\n",
    "    return targets\n",
    "\n",
    "# Augmenting training data\n",
    "def build_controlled_augmented_train_df(\n",
    "    train_df: pd.DataFrame,\n",
    "    seed: int = 42,\n",
    "    balance_strength: float = 1.0,\n",
    "    max_growth: float = 10.0,\n",
    "):\n",
    "    rng = random.Random(seed)\n",
    "\n",
    "    base = train_df[[\"text\", \"label\"]].copy().reset_index(drop=True)\n",
    "    base[\"is_augmented\"] = 0\n",
    "\n",
    "    class_counts = base[\"label\"].value_counts().sort_index()\n",
    "    target_counts = _compute_target_counts(class_counts, balance_strength=balance_strength, max_growth=max_growth)\n",
    "\n",
    "    parts = []\n",
    "    for label, grp in base.groupby(\"label\", sort=True):\n",
    "        grp = grp.copy().reset_index(drop=True)\n",
    "        originals = grp[\"text\"].tolist()\n",
    "        seen = set(t.strip().lower() for t in originals)\n",
    "\n",
    "        need = max(0, target_counts[int(label)] - len(grp))\n",
    "        new_rows = []\n",
    "        attempts = 0\n",
    "        max_attempts = max(200, need * 20)\n",
    "\n",
    "        while len(new_rows) < need and attempts < max_attempts:\n",
    "            src = originals[rng.randrange(len(originals))]\n",
    "            aug = augment_text_label_aware(src, int(label), rng, max_repl=2, swap_prob=0.10)\n",
    "            attempts += 1\n",
    "\n",
    "            key = aug.strip().lower()\n",
    "            if not key or key in seen:\n",
    "                continue\n",
    "\n",
    "            seen.add(key)\n",
    "            new_rows.append({\"text\": aug, \"label\": int(label), \"is_augmented\": 1})\n",
    "\n",
    "        if len(new_rows) < need:\n",
    "            remain = need - len(new_rows)\n",
    "            sampled = grp.sample(n=remain, replace=True, random_state=seed)[\"text\"].tolist()\n",
    "            for src in sampled:\n",
    "                aug = augment_text_label_aware(src, int(label), rng, max_repl=1, swap_prob=0.05)\n",
    "                new_rows.append({\"text\": aug, \"label\": int(label), \"is_augmented\": 1})\n",
    "\n",
    "        parts.append(grp)\n",
    "        if new_rows:\n",
    "            parts.append(pd.DataFrame(new_rows))\n",
    "\n",
    "    train_aug_df = pd.concat(parts, ignore_index=True)\n",
    "    train_aug_df = train_aug_df.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
    "    return train_aug_df\n",
    "\n",
    "\n",
    "def show_counts(train_part, val_part, test_part, title):\n",
    "    print(f\"\\n{title}\")\n",
    "    print(\"=\" * len(title))\n",
    "    for name, part in [(\"Train\", train_part), (\"Validation\", val_part), (\"Test\", test_part)]:\n",
    "        c = part[\"label\"].value_counts().sort_index().to_dict()\n",
    "        print(f\"{name}: n={len(part)} | class_counts={c}\")\n",
    "\n",
    "\n",
    "assert \"train_df\" in globals() and \"val_df\" in globals() and \"test_df\" in globals(), \"run data spliting block first\"\n",
    "\n",
    "seed_value = SEED if \"SEED\" in globals() else 42\n",
    "\n",
    "show_counts(train_df, val_df, test_df, \"Before Augmentation\")\n",
    "\n",
    "train_aug_df = build_controlled_augmented_train_df(\n",
    "    train_df=train_df,\n",
    "    seed=seed_value,\n",
    "    balance_strength=1,\n",
    "    max_growth=3,\n",
    ")\n",
    "\n",
    "show_counts(train_aug_df, val_df, test_df, \"After Augmentation\")\n",
    "print(f\"Added train samples: {len(train_aug_df) - len(train_df)}\")\n",
    "print(\"Augmented flag:\", train_aug_df[\"is_augmented\"].value_counts().to_dict())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
